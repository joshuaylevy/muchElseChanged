{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preamble and packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import partial\n",
    "print(\"Script is running, please be patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting links to each player's page\n",
    "# Page has link to each position-page\n",
    "positionRoot = \"https://overthecap.com/contract-history/\"\n",
    "positionStemVect = [\"quarterback\",\"running-back\", \"fullback\",\"wide-receiver\",\"tight-end\",\"left-tackle\",\"left-guard\",\"center\",\"right-guard\",\"right-tackle\",\"interior-defensive-line\",\"edge-rusher\",\"linebacker\",\"safety\",\"cornerback\",\"kicker\",\"punter\",\"long-snapper\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty pandas dataframe, to which each a df of players (by position) will be concatenated\n",
    "playerPagedf = pd.DataFrame(columns=['name', 'link', 'position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Scrape each position page for a dataframe of players.\n",
    "for stem in positionStemVect:\n",
    "    print(\"attempting: \"+ stem)\n",
    "     # Create unique URL for each position page\n",
    "    url = positionRoot + stem\n",
    "    \n",
    "    # load page html\n",
    "    page = requests.get(url)\n",
    "    # Parse html to be readable - This script uses the html.parser parser which is slower than lxml's parser but does not require additional dependencies or installs\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # The \"table\" has the \"position-table\" CSS class\n",
    "    table = soup.find(class_='position-table')\n",
    "\n",
    "    #initialize vectors for each variable of interest\n",
    "    player_names = []\n",
    "    player_page_links = []\n",
    "    player_position = []\n",
    "\n",
    "    # Identify the non-header rows of the table\n",
    "    tableBody = table.find('tbody')\n",
    "    tableRows = tableBody.find_all('tr')\n",
    "\n",
    "    # For each non-header row, collect the link and name\n",
    "    for row in tableRows:\n",
    "        playerObject = row.find('td')\n",
    "        #get the 'href' attribute from the first 'a' tagged element\n",
    "        playerLinkStem = playerObject.find('a').get('href')\n",
    "        playerFullURL = \"https://overthecap.com\" + playerLinkStem\n",
    "        #get the string contents of the first column of the given row\n",
    "        playerName = playerObject.string\n",
    "\n",
    "        # Insert the name, link, and position into the vectors of interest\n",
    "        player_names.append(playerName)\n",
    "        player_page_links.append(playerFullURL)\n",
    "        player_position.append(stem)\n",
    "\n",
    "    # Format the above vectors as a temporary pandas dataframe\n",
    "    tempPositiondf = pd.DataFrame({\n",
    "        'name' : player_names,\n",
    "        'link' : player_page_links,\n",
    "        'position' : player_position\n",
    "    })\n",
    "\n",
    "    # Row-bind temporary (position) df to master (all-players) df\n",
    "    playerPagedf = pd.concat([playerPagedf, tempPositiondf], ignore_index=True)\n",
    "    print(\"completed: \" + stem)\n",
    "\n",
    "#Initailize a long, player-year-contract-level dataframe\n",
    "playerContractdf = pd.DataFrame()\n",
    "\n",
    "#Initailize a vector to store broken pages for future checking\n",
    "breakPages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Scraping each player's contract history data\n",
    "playerPagedf = playerPagedf.drop_duplicates()\n",
    "\n",
    "#Drop some dead pages\n",
    "#playerPagedf = playerPagedf[playerpagedf.name != \"\"]\n",
    "playerPagedf = playerPagedf[playerpagedf.name != \"Manase Hungalu\"]\n",
    "playerPagedf = playerPagedf[playerPagedf.name != \"Cyrus Jones\"]\n",
    "\n",
    "#These are names/observations that have already been stored because they were scraped on previous runs of this block\n",
    "def extract(list):\n",
    "    return [item[0] for item in list]\n",
    "\n",
    "already_names = list(playerPagedf['name'])\n",
    "breaked_names = list(set(extract(breakPages)))\n",
    "skip_names = already_names + breaked_names\n",
    "\n",
    "\n",
    "#Begin looping over pages to scrape\n",
    "nrows = playerPagedf.shape[0]\n",
    "i=1\n",
    "for index, player_row in playerPagedf.iterrows():\n",
    "    print(\"name: \" + player_row['name'] + \" \" + str(i) + \"/\" + str(nrows) )\n",
    "\n",
    "    if player_row['name'] in skip_names:\n",
    "        print(\"already scraped \" + player_row['name'] + \": \" + str(i) + \"/\" + str(nrows))\n",
    "        i = i +1\n",
    "        continue\n",
    "    else:\n",
    "\n",
    "        # Create unique URL for each position page\n",
    "        url = player_row['link']\n",
    "        \n",
    "        # load page html\n",
    "        page = requests.get(url)\n",
    "        # Parse html to be readable - This script uses the html.parser parser which is slower than lxml's parser but does not require additional dependencies or installs\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        #Go to the contract history tab section\n",
    "        history_tab = soup.find(id='contract-history')\n",
    "        history_table = history_tab.find(class_='salary-cap-history')\n",
    "        #Identify what kind of columns are present/absent by looking into the header\n",
    "        \n",
    "        try:\n",
    "            history_table_header = history_table.find('thead').find('tr').find_all('th')\n",
    "        except AttributeError:\n",
    "            breakPages.append([player_row['name'], player_row['link']])\n",
    "            print(\"broken/NA pages: \" + str(len(breakPages)))\n",
    "            i = i +1\n",
    "            continue \n",
    "        \n",
    "        \n",
    "        #initialize vector for column titles\n",
    "        temp_header_titles = []\n",
    "        for col in history_table_header:\n",
    "            #To return the only vector in a vect of vectors list then take the first index\n",
    "            header_col = col.attrs\n",
    "            header_col_values = header_col.values()\n",
    "            \n",
    "            #Treat column headers with no attributes differently (\"Team\" col)\n",
    "            if len(header_col_values) == 0:\n",
    "                header_col_attrs = list(header_col_values)\n",
    "            else:\n",
    "                header_col_attrs = list(header_col_values)[0]\n",
    "            \n",
    "            #Omit the spacer columns\n",
    "            if 'spacer' in header_col_attrs:\n",
    "                pass\n",
    "            else:\n",
    "                header_col_title = col.string\n",
    "                temp_header_titles.append(header_col_title)\n",
    "\n",
    "        #initalize vector for actual contract data (each year is a row) so that we produce a vector of vectors (that is, a vector of individual years for each player)\n",
    "        player_year_values = []\n",
    "        history_table_rows = history_table.find('tbody').find_all('tr')\n",
    "\n",
    "        #going year-by-year\n",
    "        for row in history_table_rows:\n",
    "            #initialize year vector\n",
    "            year_values = []\n",
    "\n",
    "            #find each cell for each row\n",
    "            cols = row.find_all('td')\n",
    "            #length adjustment\n",
    "            for col in cols:\n",
    "                attrs = col.attrs\n",
    "                if len(attrs.values()) == 0:\n",
    "                    col_attrs = list(attrs.values())\n",
    "                else:\n",
    "                    col_attrs = list(attrs.values())[0]\n",
    "                \n",
    "                # Omit spacer cells\n",
    "                if 'spacer' in col_attrs:\n",
    "                    pass\n",
    "                else: \n",
    "                    col_value = col.string\n",
    "                    #append value to the year vector\n",
    "                    year_values.append(col_value)\n",
    "\n",
    "            player_year_values.append(year_values)\n",
    "        \n",
    "        #store the header titles (column names) and values (immediately aboce) in a df\n",
    "        playerdf = pd.DataFrame(\n",
    "            player_year_values,\n",
    "            columns=temp_header_titles\n",
    "        )\n",
    "        #Adding names and positions back to the player-specific df\n",
    "        playerdf['name'] = player_row['name']\n",
    "        playerdf['position'] = player_row['position']\n",
    "\n",
    "        #row bind the player-specific df to the full df (playerContractdf)\n",
    "        #pandas takes care of differences in columns by merging on the column name (same by virtue of scrape) and including new columns (reassigning missing values to 'NaN')\n",
    "        playerContractdf = pd.concat([playerContractdf, playerdf], ignore_index=True)  \n",
    "        i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up scraped data for export and processing\n",
    "playerContractdf.columns = playerContractdf.columns.fillna('cap_number')\n",
    "playerContractdf = playerContractdf.rename(columns={\"Year\" : \"year\",\n",
    "    \"Team\" : \"team\",\n",
    "    \"Base Salary\" : \"base_salary\",\n",
    "    \"Prorated Bonus\" : \"prorated_bonus\",\n",
    "    \"Roster Bonus\" : \"roster_bonus\",\n",
    "    \"Workout Bonus\" : \"workout_bonus\",\n",
    "    \"Guaranteed Salary\" : \"guaranteed_salary\",\n",
    "    \"Cap %\" : \"cap_percent\",\n",
    "    \"Cash Paid\" : 'cash_paid',\n",
    "    \"Other Bonus\" : \"other_bonus\",\n",
    "    \"Per Game Roster Bonus\" : \"per_game_ros_bonus\",\n",
    "    \"name\" : \"name\",\n",
    "    'position' : 'position' \n",
    "})\n",
    "\n",
    "vars = ['base_salary', 'prorated_bonus', 'roster_bonus', 'workout_bonus', 'guaranteed_salary', 'cap_number', 'cap_percent', 'cash_paid', 'other_bonus', 'per_game_ros_bonus']\n",
    "playerContractdf[vars] = playerContractdf[vars].replace({'\\$' : '', ',' : '', '\\%': ''}, regex=True)\n",
    "\n",
    "\n",
    "\n",
    "playerContractdf.to_csv(r'total_export.csv', header=True, index=False)\n"
   ]
  }
 ]
}